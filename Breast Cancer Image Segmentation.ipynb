{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install tf_explain\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.image as tfi\n",
    "\n",
    "# Data\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Data Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model \n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Add\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Callbacks \n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "# Metrics\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de181122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image, SIZE):\n",
    "    return np.round(tfi.resize(img_to_array(load_img(image))/255.,(SIZE, SIZE)),4)\n",
    "\n",
    "def load_images(image_paths, SIZE, mask=False, trim=None):\n",
    "    if trim is not None:\n",
    "        image_paths = image_paths[:trim]\n",
    "    \n",
    "    if mask:\n",
    "        images = np.zeros(shape=(len(image_paths), SIZE, SIZE, 1))\n",
    "    else:\n",
    "        images = np.zeros(shape=(len(image_paths), SIZE, SIZE, 3))\n",
    "    \n",
    "    for i,image in enumerate(image_paths):\n",
    "        img = load_image(image,SIZE)\n",
    "        if mask:\n",
    "            images[i] = img[:,:,:1]\n",
    "        else:\n",
    "            images[i] = img\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=None, cmap=None, alpha=1):\n",
    "    plt.imshow(image, cmap=cmap, alpha=alpha)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "def show_mask(image, mask, cmap=None, alpha=0.4):\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(tf.squeeze(mask), cmap=cmap, alpha=alpha)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\n",
    "classes = sorted(os.listdir(root_path))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mask_paths = sorted([sorted(glob(root_path + name + \"/*mask.png\")) for name in classes])\n",
    "double_mask_paths = sorted([sorted(glob(root_path + name + \"/*mask_1.png\")) for name in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "mask_paths = []\n",
    "for class_path in single_mask_paths:\n",
    "    for path in class_path:\n",
    "        img_path = path.replace('_mask','')\n",
    "        image_paths.append(img_path)\n",
    "        mask_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(load_image(image_paths[0], SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b042f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mask(load_image(image_paths[0], SIZE), load_image(mask_paths[0], SIZE)[:,:,0], alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((1,SIZE,SIZE,3))\n",
    "mask1 = load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask_1.png', SIZE)\n",
    "mask2 = load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask.png', SIZE)\n",
    "\n",
    "img = img + mask1 + mask2\n",
    "img = img[0,:,:,0]\n",
    "show_image(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, cmap='binary', alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, cmap='gray', alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a452aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b863318",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(image_paths, SIZE)\n",
    "masks = load_images(mask_paths, SIZE, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89752cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='jet')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48437e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='binary')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='afmhot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='copper')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c266825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tf.keras.layers.Input((shape=images.shape[-3:]))\n",
    "#shuffle data\n",
    "#from sklearn.utils import shuffle\n",
    "#X_train,y_train = shuffle(X_train,y_train)\n",
    "\n",
    "input_layer = Input(shape=images.shape[-3:])\n",
    "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "#Contraction path\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    " \n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    " \n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    " \n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "#Expansive path \n",
    "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    " \n",
    "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    " \n",
    "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    " \n",
    "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3),name=\"Attention4\", activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    " \n",
    "outputs = tf.keras.layers.Conv2D(1, (1, 1),padding='same', activation='sigmoid')(c9)\n",
    " \n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', MeanIoU(num_classes=2, name='IoU')])\n",
    "\n",
    "cb = [\n",
    "    # EarlyStopping(patience=3, restore_best_weight=True), # With Segmentation I trust on eyes rather than on metrics\n",
    "    ModelCheckpoint(\"AttentionCustomUNet.h5\", save_best_only=True),\n",
    "    ShowProgress()\n",
    "]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217eb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress(Callback):\n",
    "    def on_epoch_end(self, epochs, logs=None):\n",
    "        id = np.random.randint(200)\n",
    "        exp = GradCAM()\n",
    "        image = images[id]\n",
    "        mask = masks[id]\n",
    "        pred_mask = self.model.predict(image[np.newaxis,...])\n",
    "        cam = exp.explain(\n",
    "            validation_data=(image[np.newaxis,...], mask),\n",
    "            class_index=1,\n",
    "            layer_name='Attention4',\n",
    "            model=self.model\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title(\"Original Mask\")\n",
    "        show_mask(image, mask, cmap='copper')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        show_mask(image, pred_mask, cmap='copper')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        show_image(cam,title=\"GradCAM\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "SPE = len(images)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "results = model.fit(\n",
    "    images, masks,\n",
    "    validation_split=0.2,\n",
    "    epochs=20, # 15 will be enough for a good Model for better model go with 20+\n",
    "    steps_per_epoch=SPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, iou, val_loss, val_accuracy, val_iou = results.history.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Model Loss\")\n",
    "plt.plot(loss, label=\"Training\")\n",
    "plt.plot(val_loss, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.plot(accuracy, label=\"Training\")\n",
    "plt.plot(val_accuracy, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Model IoU\")\n",
    "plt.plot(iou, label=\"Training\")\n",
    "plt.plot(val_iou, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25))\n",
    "n=0\n",
    "for i in range(1,(5*3)+1):\n",
    "    plt.subplot(5,3,i)\n",
    "    if n==0:\n",
    "        id = np.random.randint(len(images))\n",
    "        image = images[id]\n",
    "        mask = masks[id]\n",
    "        pred_mask = model.predict(image[np.newaxis,...])\n",
    "\n",
    "        plt.title(\"Original Mask\")\n",
    "        show_mask(image, mask)\n",
    "        n+=1\n",
    "    elif n==1:\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        show_mask(image, pred_mask)\n",
    "        n+=1\n",
    "    elif n==2:\n",
    "        pred_mask = (pred_mask>0.5).astype('float')\n",
    "        plt.title(\"Processed Mask\")\n",
    "        show_mask(image, pred_mask)\n",
    "        n=0\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def738b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073946be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91913fac",
   "metadata": {},
   "source": [
    "Kaggle dataset: https://www.kaggle.com/datasets/utkarshsaxenadn/breast-cancer-detection-unet/data\n",
    " Link ref:        https://www.kaggle.com/code/ahmedkhairullah/breast-cancer-image-segmentation-attention-unet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
